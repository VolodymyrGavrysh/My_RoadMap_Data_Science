{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from dask_ml.preprocessing import StandardScaler\n",
    "    import gc\n",
    "    import time\n",
    "    import dask.dataframe as dd\n",
    "    from dask.distributed import Client, progress # see the sys load\n",
    "    \n",
    "except: ImportError(), 'Some modules have not loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/volodymyr/anaconda3/lib/python3.7/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:35867</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:33551/status' target='_blank'>http://127.0.0.1:33551/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>6.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35867' processes=2 threads=4, memory=6.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set workers\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='3GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of rows for the CSV file\n",
    "start_time = time.time()\n",
    "\n",
    "N = 5_000_000\n",
    "columns = 257\n",
    "\n",
    "# create DF \n",
    "df = pd.DataFrame(np.random.randint(999, 999999, size=(N, columns)), columns=['level_%s' % i for i in range(0, columns)])\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to csv \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df.to_csv('random.csv', sep=',')\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 877.5422155857086 seconds, 8.9 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gib = 'random.csv' # 5 mln records! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBigCsvFile_TRAIN:\n",
    "    \n",
    "    '''load data from tsv, transform, scale, add two columns'''\n",
    "    \n",
    "    def __init__(self, train, scaler=StandardScaler(copy=False)):\n",
    "\n",
    "        self.train = train\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def read_data(self):\n",
    "        # use dask for read with C \n",
    "        try:\n",
    "            data_train = dd.read_csv(self.train, sep=',', header=None, skiprows=1, \\\n",
    "                                     dtype={n:'int16' for n in range(1, 300)}, engine='c')            \n",
    "        except: MemoryError, 'not enough memory'\n",
    "        \n",
    "        # fit from train and scale to test\n",
    "        self.scaler.fit(data_train.iloc[:,1:])\n",
    "        temp = self.scaler.transform(data_train.iloc[:,1:])\n",
    "                \n",
    "        # index of max element\n",
    "        temp['max_feature_2_index'] = temp.idxmax(axis=1)\n",
    "        \n",
    "        # calculate absolute diviation from max value in row\n",
    "        temp['max_feature_2_abs_mean_diff'] = abs(temp.max(axis=1) - temp.mean(axis=1))\n",
    "        \n",
    "        # set id columnsD\n",
    "        temp['job_id'] = data_train.iloc[:,0] \n",
    "        \n",
    "        del data_train # del not needed\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''init the class,'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rer = LoadBigCsvFile_TRAIN(gib, StandardScaler(copy=False)).read_data()\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 10 ces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to HDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "rer.to_hdf('random.hdf',  key='df1')\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) #152.77526926994324 seconds!!! 10.4 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from HDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hdf_read = dd.read_hdf('random.hdf', key='df1', mode='r', chunksize=10000)\n",
    "\n",
    "print('%s seconds' % (time.time() - start_time)) # 0.03516435623168945 seconds!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_read.head(3) # read head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(hdf_read) == 5_000_000 # test 5ml records"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
